{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT210x - Programming with Python for DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module5- Lab10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.utils.validation import check_random_state\n",
    "import scipy.io.wavfile as wavfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good Luck! Heh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples are Observations. Each audio file will is a single sample in our dataset.\n",
    "\n",
    "Find more information about [Audio Samples here](https://en.wikipedia.org/wiki/Sampling_(signal_processing)).\n",
    "\n",
    "Each .wav file is actually just a bunch of numeric samples, \"sampled\" from the analog signal. Sampling is a type of discretization. When we mention 'samples', we mean observations. When we mention 'audio samples', we mean the actually \"features\" of the audio file.\n",
    "\n",
    "The goal of this lab is to use multi-target, linear regression to generate by extrapolation, the missing portion of the test audio file.\n",
    "\n",
    "Each one audio_sample features will be the output of an equation, which is a function of the provided portion of the audio_samples:\n",
    "\n",
    "    missing_samples = f(provided_samples)\n",
    "\n",
    "You can experiment with how much of the audio you want to chop off and have the computer generate using the Provided_Portion parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with this. This is how much of the audio file will be provided, in percent. The remaining percent of the file will be generated via linear extrapolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Provided_Portion = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You have to download the dataset (audio files) from the website: https://github.com/Jakobovski/free-spoken-digit-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by creating a regular Python List called `zero`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through the dataset and load up all 50 of the `0_jackson*.wav` files using the `wavfile.read()` method: https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.io.wavfile.read.html Be careful! `.read()` returns a tuple and you're only interested in the audio data, and not sample_rate at this point. Inside your for loop, simply append the loaded audio data into your Python list `zero`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "zero = []\n",
    "\n",
    "import os\n",
    "for file in os.listdir('Datasets/free-spoken-digit-dataset-master/recordings'):\n",
    "    if file.startswith('0_jackson'):\n",
    "        a = os.path.join('Datasets/free-spoken-digit-dataset-master/recordings', file)\n",
    "        sample_rate, audio_data = wavfile.read(a)\n",
    "        zero.append(audio_data)\n",
    "print (len(zero)) # 50, as expected since there 50 files in the folder starting with \"0_jackson\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for a second, convert zero into a DataFrame. When you do so, set the `dtype` to `np.int16`, since the input audio files are 16 bits per sample. If you don't know how to do this, read up on the docs here: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\n",
    "\n",
    "Since these audio clips are unfortunately not length-normalized, we're going to have to just hard chop them to all be the same length. Since Pandas would have inserted NANs at any spot to make zero a  perfectly rectangular [n_observed_samples, n_audio_samples] array, do a `dropna` on the Y axis here. Then, convert one back into an NDArray using `yourarrayname.values`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0     1     2     3     4     5     6     7     8     9     ...   4077  \\\n",
      "0   -369  -431  -475  -543  -571  -557  -528  -455  -394  -305  ...   2475   \n",
      "1   -311   -91  -140  -182  -271   -68  -235  -359  -129  -198  ...   -122   \n",
      "2   -314  -303  -332  -376  -344  -344  -334  -295  -243  -224  ...  -2084   \n",
      "3    347   351   462   451   499   509   469   534   362   464  ...   1163   \n",
      "4   -336   160    65  -161   -23    61    39  -332   348  -198  ...    105   \n",
      "5    354   442   610   728   768   842   873   784   739   696  ...    857   \n",
      "6    397   531   638   699   744   769   732   694   601   508  ...   2162   \n",
      "7    382   459   530   591   641   672   690   630   592   471  ...   -501   \n",
      "8   -393    54   -71  -370   -75    39  -234  -308    79  -155  ...     21   \n",
      "9   -311  -363  -318  -476  -474  -500  -452  -393  -400  -329  ...   -327   \n",
      "10  -316  -336  -342  -351  -358  -369  -357  -331  -302  -279  ...  -1365   \n",
      "11   335   392   481   519   597   604   592   623   501   473  ...   -805   \n",
      "12  -361  -226  -238  -478  -425  -395  -663  -879  -726  -997  ...   -141   \n",
      "13  -309  -323  -333  -335  -332  -340  -334  -342  -328  -326  ...    163   \n",
      "14   305   305   294   288   271   252   231   210   191   171  ...    842   \n",
      "15   342   452   546   612   664   697   699   677   622   557  ...    580   \n",
      "16  -350  -391  -371  -334  -228   -86    95   281   484   654  ...   2376   \n",
      "17  -358  -447  -533  -615  -693  -754  -804  -834  -819  -757  ...    648   \n",
      "18  -313  -347  -332  -316  -290  -268  -260  -221  -182  -171  ...   1862   \n",
      "19   332     0   -58   -40     7    -7  -121   135  -186    83  ...    -18   \n",
      "20  -304  -319  -377  -432  -484  -556  -620  -645  -698  -745  ...  -3403   \n",
      "21  -332  -396  -502  -607  -730  -890 -1023 -1110 -1141 -1110  ...   -826   \n",
      "22  -317  -346  -377  -426  -468  -488  -496  -498  -460  -387  ...    900   \n",
      "23   323   338   357   388   381   393   391   378   348   313  ...   1323   \n",
      "24  -328  -401  -471  -556  -661  -761  -831  -897  -963  -986  ...   -528   \n",
      "25  -328  -372  -403  -431  -411  -328  -275  -186   -56   111  ...    248   \n",
      "26  -310  -377  -502  -520  -639  -621  -532  -487  -341  -180  ...     55   \n",
      "27   310   339   351   368   365   362   379   354   325   326  ...   2012   \n",
      "28  -309  -385  -459  -607  -713  -788  -849  -883  -863  -782  ...   2216   \n",
      "29   303   348   354   380   396   386   384   354   410   314  ...    380   \n",
      "30  -348  -418  -431  -487  -561  -553  -492  -457  -367  -320  ...   1363   \n",
      "31   306   399   455   487   552   606   620   594   603   572  ...   -713   \n",
      "32  -326  -362  -376  -418  -490  -485  -483  -491  -440  -395  ...     83   \n",
      "33  -317  -199  -303  -190  -233   -24   -76    74    61   224  ...   -750   \n",
      "34  -302  -312  -103  -305  -145  -339    12  -433  -141  -189  ...    189   \n",
      "35   322   395   470   529   559   595   598   589   600   551  ...  -2512   \n",
      "36   355   425   481   505   529   533   532   524   509   520  ...  -4633   \n",
      "37   331   404   490   557   617   664   687   682   687   674  ...  -1803   \n",
      "38  -307  -334  -336  -325  -339  -348  -330  -307  -301  -302  ...   -446   \n",
      "39  -316  -377  -435  -504  -547  -597  -636  -649  -656  -665  ...   4744   \n",
      "40   301   394   507   602   685   780   865   931   980   995  ...   4373   \n",
      "41   343   404   454   477   477   462   425   393   364   311  ...  -3480   \n",
      "42   320   372   421   468   514   544   564   576   581   565  ...  -1751   \n",
      "43  -439  -572  -656  -709  -743  -743  -730  -714  -740  -784  ...    -81   \n",
      "44  -302  -337  -371  -416  -463  -533  -589  -580  -577  -587  ...  -4584   \n",
      "45   305   365   419   486   540   594   602   624   603   576  ...   -735   \n",
      "46   364   420   469   529   557   575   574   533   467   385  ...   1925   \n",
      "47  -417   152   168  -176   -48   243   -83   -77   159   -14  ...    594   \n",
      "48   330   382   389   401   429   426   390   405   379   338  ...    117   \n",
      "49  -312  -335  -338  -344  -338  -332  -303  -261  -233  -197  ...  -1537   \n",
      "\n",
      "    4078  4079  4080  4081  4082  4083  4084  4085  4086  \n",
      "0   2500  2331  1786  1057   358   158  -108  -402  -884  \n",
      "1   -207  -266  -276  -316  -359  -396  -422  -462  -460  \n",
      "2  -1223  -226   960  2267  3264  3553  3418  2640  1202  \n",
      "3   1075   831   269  -419 -1247 -2099 -2615 -2777 -2561  \n",
      "4     98   118    99    64   -73  -181  -315  -343  -319  \n",
      "5    844   960  1056  1030   820   491   263   223   385  \n",
      "6   2901  2884  2282  1520   689   -39  -328  -161   -28  \n",
      "7    140   829  1408  1649  1341   642  -206  -920 -1501  \n",
      "8     41    51    32    -3   -36   -88  -215  -325  -448  \n",
      "9   -427  -459  -429  -424  -400  -333  -341  -291  -254  \n",
      "10 -1591 -1593 -1439 -1021  -358   311   488   754  1196  \n",
      "11 -1075 -1343 -1464 -1170  -783  -605  -605  -630  -559  \n",
      "12   -83     0    71   165   284   325   378   416   438  \n",
      "13   100   -98  -472  -811  -996  -914  -492   185   634  \n",
      "14   810   631   507   424   323   202   158    81    45  \n",
      "15   589   781  1233  1668  1926  1890  1605  1141   653  \n",
      "16  1353   255  -723 -1581 -2417 -3038 -3221 -2688 -1822  \n",
      "17  1145  1387  1116   761   377    10  -476  -695  -583  \n",
      "18  1998  1933  1590  1256  1029  1038  1103  1257  1115  \n",
      "19   -39   427  1066  1399  1479  1502   984   563   442  \n",
      "20 -2657 -1254  -330   433   876  1231  1539  1267  1009  \n",
      "21 -1113 -1398 -1047 -1021 -1007  -913  -256   713  1265  \n",
      "22  1128  1356  1364  1125   836   528   240   -74  -313  \n",
      "23  1321  1211   827   292  -244  -779 -1241 -1369 -1229  \n",
      "24 -1264 -1474  -935   -13   474   745   752  1101  1478  \n",
      "25   362   468   412   252    60  -126  -187   -45   169  \n",
      "26  -497  -980 -1445 -1771 -1718 -1413  -901  -177   484  \n",
      "27  1860  1761  1735  2110  2419  1697  -204 -2665 -5532  \n",
      "28  2909  2762  2286  1645  1377  1379   724   163  -433  \n",
      "29    43  -107   196   831  1462  1748  1603  1150   525  \n",
      "30  1083   563   -63  -725 -1255 -1452 -1380 -1008  -652  \n",
      "31 -1112 -1354 -1200  -695  -148   145    73  -153  -128  \n",
      "32    75   -45   -21  -502 -1290 -2550 -3727 -4336 -5194  \n",
      "33  -772  -489   -90    68   117   252   436   687   984  \n",
      "34   211   189   134   150   224   265   313   328   327  \n",
      "35 -2680 -2378  -931   558   864   990  1116  2002  2234  \n",
      "36 -2683   435  3005  3932  3671  3187  3987  5487  6692  \n",
      "37 -2059  -614   815  1787  2722  3698  4225  4683  5310  \n",
      "38  -668  -981 -1299 -1390 -1206  -837  -279   421   898  \n",
      "39  4043  3913  4118  4264  3645  2410   -95 -3212 -5364  \n",
      "40  3226  4336  5008  6301  6111  5031  2471 -1658 -3648  \n",
      "41 -3396 -2780 -1489   335  1496  1456  2000  3001  3651  \n",
      "42 -2016 -2324 -1977 -1095    70   965  1433  1616  1427  \n",
      "43  -363 -1570 -2769 -2494 -1677  -315  1026  2322  2618  \n",
      "44 -3857 -2405  -107  1812  2723  2095  1105   559   477  \n",
      "45  -679  -526  -315   103   675  1051  1087   770   403  \n",
      "46  1059   468    27  -380  -534  -631  -697  -714  -898  \n",
      "47   663   670   724   788   820   770   638   463   332  \n",
      "48   -88  -238  -253  -110   111   375   602   708   701  \n",
      "49 -1499 -1398 -1143  -857  -583   -31   644   815   580  \n",
      "\n",
      "[50 rows x 4087 columns]\n",
      "<class 'numpy.ndarray'>\n",
      "[[-369 -431 -475 ..., -108 -402 -884]\n",
      " [-311  -91 -140 ..., -422 -462 -460]\n",
      " [-314 -303 -332 ..., 3418 2640 1202]\n",
      " ..., \n",
      " [-417  152  168 ...,  638  463  332]\n",
      " [ 330  382  389 ...,  602  708  701]\n",
      " [-312 -335 -338 ...,  644  815  580]]\n"
     ]
    }
   ],
   "source": [
    "zero = pd.DataFrame(data = zero, dtype = np.int16)\n",
    "zero.dropna(axis = 1, inplace = True)\n",
    "print (zero)\n",
    "zero = zero.values\n",
    "print (type(zero)) # zero is now a numpy NDArray\n",
    "print (zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to know how (many audio_samples samples) long the data is now.\n",
    "\n",
    "`zero` is currently shaped like `[n_samples, n_audio_samples]`, so get the `n_audio_samples` count and store it in a variable called `n_audio_samples`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4087\n"
     ]
    }
   ],
   "source": [
    "n_audio_samples = zero.shape[1]\n",
    "print (n_audio_samples) #4087\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your linear regression model here and store it in a variable called `model`. Don't actually train or do anything else with it yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "model = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 50 takes of each clip. You want to pull out just one of them, randomly, and that one will NOT be used in the training of your model. In other words, the one file we'll be testing / scoring on will be an unseen sample, independent to the rest of your training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this line alone until you've submitted your lab:\n",
    "from sklearn.utils.validation import check_random_state\n",
    "rng = check_random_state(7)\n",
    "\n",
    "random_idx = rng.randint(zero.shape[0])\n",
    "test  = zero[random_idx]\n",
    "train = np.delete(zero, [random_idx], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the shape of `train`, and the shape of `test`.\n",
    "\n",
    "`train` will be shaped: `[n_samples, n_audio_samples]`, where `n_audio_samples` are the 'features' of the audio file \n",
    "\n",
    "`test` will be shaped `[n_audio_features]`, since it is a single sample (audio file, e.g. observation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of train and test, respectively: (49, 4087) (4087,)\n"
     ]
    }
   ],
   "source": [
    "print (\"Shapes of train and test, respectively:\", train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The test data will have two parts, `X_test` and `y_test`.\n",
    "\n",
    "`X_test` is going to be the first portion of the test audio file, which we will be providing the computer as input. \n",
    "\n",
    "`y_test`, the \"label\" if you will, is going to be the remaining portion of the audio file. Like such, the computer will use linear regression to derive the missing portion of the sound file based off of the training data its received!\n",
    "\n",
    "Let's save the original `test` clip, the one you're about to delete half of, to the current directory so that you can compare it to the 'patched' clip once you've generated it. You should have already got the `sample_rate` when you were loading up the .wav files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wavfile.write('Original Test Clip.wav', sample_rate, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the TEST data by creating a slice called `X_test`. It should have `Provided_Portion` * `n_audio_samples` audio sample features, taken from your test audio file, currently stored in variable `test`. In other words, grab the FIRST `Provided_Portion` * `n_audio_samples` audio features from `test` and store it in `X_test`. This should be accomplished using indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -417,   152,   168, ..., -4055, -6846, -6992], dtype=int16)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = test[:int(Provided_Portion*n_audio_samples)]\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the first `Provided_Portion` * `n_audio_samples` features were stored in `X_test`, then we need to also grab the _remaining_ audio features and store them in `y_test`. With the remaining features stored in there, we will be able to R^2 \"score\" how well our algorithm did in completing the sound file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7992, -8650, -9005, ...,   638,   463,   332], dtype=int16)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = test[int(Provided_Portion*n_audio_samples):]\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate the same process for `X_train`, `y_train`. The only differences being:\n",
    "\n",
    "1. Your will be getting your audio data from `train` instead of from `test`\n",
    "2. Remember the shape of `train` that you printed out earlier? You want to do this slicing but for ALL samples (observations). For each observation, you want to slice the first `Provided_Portion` * `n_audio_samples` audio features into `X_train`, and the remaining go into `y_test`. All of this should be doable using regular indexing in two lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -369,  -431,  -475, ...,  -256,  -240, -1102],\n",
       "       [ -311,   -91,  -140, ...,   864,  1798,  3052],\n",
       "       [ -314,  -303,  -332, ...,  -193,   487,  -294],\n",
       "       ..., \n",
       "       [  364,   420,   469, ...,  -650, -1770, -2556],\n",
       "       [  330,   382,   389, ...,   993,   677,   584],\n",
       "       [ -312,  -335,  -338, ...,  3429,  5376,  6313]], dtype=int16)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train[:, :int(Provided_Portion*n_audio_samples)]\n",
    "y_train = train[:, int(Provided_Portion*n_audio_samples):]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciKit-Learn gets 'angry' if you don't supply your training data in the form of a 2D dataframe shaped like `[n_samples, n_features]`.\n",
    "\n",
    "So if you only have one SAMPLE, such as is our case with `X_test`, and `y_test`, then by calling `.reshape(1, -1)`, you can turn `[n_features]` into `[1, n_features]` in order to appease SciKit-Learn.\n",
    "\n",
    "On the other hand, if you only have one FEATURE, you can alternatively call `.reshape(-1, 1)` on your data to turn `[n_samples]` into `[n_samples, 1]`.\n",
    "\n",
    "Reshape X_test and y_test as [1, n_features]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -417,   152,   168, ..., -4055, -6846, -6992]], dtype=int16)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test.reshape(1, -1)\n",
    "y_test = y_test.reshape(1, -1)\n",
    "X_test                    #Check to see the amount of brackets that has increased by 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit your model using your training data and label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your model to predict the `label` of `X_test`. Store the resulting prediction in a variable called `y_test_prediction`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciKit-Learn will use float64 to generate your predictions so let's take those values back to int16, which is what our .wav files expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_prediction = y_test_prediction.astype(dtype=np.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score how well your prediction would do for some good laughs, by passing in your test data and test label `y_test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrapolation R^2 Score:  0.0\n"
     ]
    }
   ],
   "source": [
    "score = model.score(X_test, y_test)\n",
    "print (\"Extrapolation R^2 Score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the first `Provided_Portion` portion of the test clip, the part you fed into your linear regression model. Then, stitch that together with the 'abomination' the predictor model generated for you and then save the completed audio clip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_clip = np.hstack((X_test, y_test_prediction))\n",
    "wavfile.write('Extrapolated Clip.wav', sample_rate, completed_clip[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats on making it to the end of this crazy lab and module =) !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "58px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
