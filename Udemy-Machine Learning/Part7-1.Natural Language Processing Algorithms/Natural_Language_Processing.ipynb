{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural Language Processing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "zMnCvemqeqoi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "d9836d14-4ff6-4dff-850f-3778b400a71c"
      },
      "cell_type": "code",
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Importing the dataset\n",
        "!curl https://raw.githubusercontent.com/muke888/UdemyMachineLearning/master/Part7-1.Natural%20Language%20Processing%20Algorithms/Restaurant_Reviews.tsv -o Restaurant_Reviews.tsv"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 61332  100 61332    0     0   356k      0 --:--:-- --:--:-- --:--:--  356k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yDO3ETMSe3iH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "7814fc8b-ac4f-47c6-b2e7-5856c57104a5"
      },
      "cell_type": "code",
      "source": [
        "# Importing the dataset\n",
        "dataset = pd.read_csv('Restaurant_Reviews.tsv',delimiter = '\\t', quoting = 3)\n",
        "dataset.info()\n",
        "dataset.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 2 columns):\n",
            "Review    1000 non-null object\n",
            "Liked     1000 non-null int64\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 15.7+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>Liked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Review  Liked\n",
              "0                           Wow... Loved this place.      1\n",
              "1                                 Crust is not good.      0\n",
              "2          Not tasty and the texture was just nasty.      0\n",
              "3  Stopped by during the late May bank holiday of...      1\n",
              "4  The selection on the menu was great and so wer...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "10CK5u0b1pJt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Refresher for regular expressions:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "'^[a-zA-Z]':  match all strings that start with a letter\n",
        "'[^a-zA-Z]':  match all strings that contain a non-letter\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AqUroYLpe3jz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "62786248-6602-4c3b-8e52-78e02475ef7c"
      },
      "cell_type": "code",
      "source": [
        "# Cleaning the texts\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "corpus = []\n",
        "ps = PorterStemmer()\n",
        "for i in range(0, 1000):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eelfwhNhe3l6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "85f536d3-1f89-4a36-b0bf-051bf911e15a"
      },
      "cell_type": "code",
      "source": [
        "corpus[:10]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wow love place',\n",
              " 'crust good',\n",
              " 'tasti textur nasti',\n",
              " 'stop late may bank holiday rick steve recommend love',\n",
              " 'select menu great price',\n",
              " 'get angri want damn pho',\n",
              " 'honeslti tast fresh',\n",
              " 'potato like rubber could tell made ahead time kept warmer',\n",
              " 'fri great',\n",
              " 'great touch']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "qnAD9Utg522u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "084fea34-e882-43d9-9790-5762c5780676"
      },
      "cell_type": "code",
      "source": [
        "# Creating the Bag of Words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#create a feature matrix out of the most 1500 frequent words:\n",
        "cv = CountVectorizer(max_features = 1500) \n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "y = dataset.iloc[:, 1].values\n",
        "X[:5]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "4CN7ytOoDgdt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "f8a339c3-e574-4213-bafe-810ce6f160a4"
      },
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
        "X_train[:5]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "okWU9xFWDgnP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "855c3955-62b6-483f-a2c6-61e157fa6429"
      },
      "cell_type": "code",
      "source": [
        "# Fitting Naive Bayes to the Training set\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "pmaW7KkiDgpd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "8f119940-ebc4-462b-d8ea-2113ad6fc831"
      },
      "cell_type": "code",
      "source": [
        "# Predicting the Test set results\n",
        "# Looking at first 5 testing data, we can see we predicted the first 3 incorrectly as positive reviews, and last 2 correctly as negative review\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "print(y_pred[:5])\n",
        "print(y_test[:5])\n",
        "print(cv.inverse_transform(X_test[:5]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1 1 0 0]\n",
            "[0 0 0 0 0]\n",
            "[array(['aw', 'food', 'present'], dtype='<U17'), array(['food', 'servic', 'worst'], dtype='<U17'), array(['dine', 'never', 'place'], dtype='<U17'), array(['disgrac', 'guess', 'mayb', 'night', 'went'], dtype='<U17'), array(['avoid', 'lover', 'mean', 'place', 'sushi'], dtype='<U17')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DHhfS_YXDgr4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "93efd098-a321-45da-f99f-13183d393066"
      },
      "cell_type": "code",
      "source": [
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[55, 42],\n",
              "       [12, 91]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "MuC3_lqlEHyI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "db48228c-2dfe-4bc3-cb0e-0b5d29fb7dc0"
      },
      "cell_type": "code",
      "source": [
        "# True Negative, False Positive, False Negative, True Positive\n",
        "# Not bad results for training on only 800 reviews\n",
        "\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(tn, fp, fn, tp)\n",
        "print(\"Accuracy of:\", (tn+tp)/(tn+tp+fp+fn))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "55 42 12 91\n",
            "Accuracy of: 0.73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VUzNlcZwXGV4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "Precision = TP / (TP + FP)\n",
        "\n",
        "Recall = TP / (TP + FN)\n",
        "\n",
        "F1 Score = 2 * Precision * Recall / (Precision + Recall)"
      ]
    },
    {
      "metadata": {
        "id": "TMIOVr5-gspU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "205066c4-db73-4e2c-d2d4-b0bf52754fa9"
      },
      "cell_type": "code",
      "source": [
        "#Examples\n",
        "#Example of PorterStemmer\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        " \n",
        "words = [\"game\",\"gamed\",\"gaming\",\"games\"]\n",
        "ps = PorterStemmer()\n",
        " \n",
        "for word in words:\n",
        "    print(ps.stem(word))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "game\n",
            "game\n",
            "game\n",
            "game\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9anGAnjNh-p7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b31c5b57-72f2-4803-eb14-8e663e3cd985"
      },
      "cell_type": "code",
      "source": [
        "#Example of tokenization\n",
        "\n",
        "import nltk\n",
        "#nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        " \n",
        "data = \"That quick black cat who jumped over the black lazy dog.\"\n",
        "words = word_tokenize(data)\n",
        "print(words)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['That', 'quick', 'black', 'cat', 'who', 'jumped', 'over', 'the', 'black', 'lazy', 'dog', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NwQTVH40jMP_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "05977eaa-fc6d-4353-ccdb-adaf8797ea63"
      },
      "cell_type": "code",
      "source": [
        "#Example of using Stopwords\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        " \n",
        "data = \"That quick black cat who jumped over the black lazy dog.\"\n",
        "stopWords = set(stopwords.words('english'))\n",
        "words = word_tokenize(data)\n",
        "wordsFiltered = []\n",
        " \n",
        "for w in words:\n",
        "    if w not in stopWords:\n",
        "        wordsFiltered.append(w)\n",
        " \n",
        "print(wordsFiltered)\n",
        "print(stopWords)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['That', 'quick', 'black', 'cat', 'jumped', 'black', 'lazy', 'dog', '.']\n",
            "{'doing', 'further', 'and', 'no', 'am', \"wouldn't\", 'below', \"you'd\", 'only', \"aren't\", 'hers', 'are', \"needn't\", \"won't\", 'wouldn', 'if', 'ma', 'yours', 'our', 'out', 'can', 'had', 'the', 'why', 'ourselves', 'at', 'were', 'she', 'was', 'own', 'shan', \"you're\", 'didn', 'as', \"hadn't\", 'mustn', 'themselves', \"wasn't\", 'couldn', 'needn', 'between', 'whom', 'off', 'isn', 'hadn', 'or', 'who', 'you', 'there', 'against', 'during', 'on', 'yourselves', 'these', 'be', 'has', 'about', 'they', 'here', 'not', \"hasn't\", 'i', 'it', 'but', 'few', \"weren't\", 'her', 'now', 'being', \"shouldn't\", 'himself', 'we', 'yourself', 'when', \"couldn't\", 'don', \"mustn't\", 'its', 'which', \"that'll\", 'again', 'does', 'very', 'mightn', 'by', 'until', 'into', \"you'll\", 'under', 'to', 'have', \"haven't\", \"don't\", 'this', 'should', 'ours', 'with', 'because', 'doesn', 'of', \"isn't\", 'herself', 'd', 'their', 'theirs', \"it's\", 'any', 'more', 'just', 'won', 'aren', 'ain', 'me', 'him', 'than', 'll', \"shan't\", 'itself', 'same', 's', 'will', 'those', 'once', 'nor', 'too', 'them', 'above', 'then', 'o', 'y', 'so', \"should've\", 'in', \"didn't\", 't', \"she's\", 'did', 'an', 'through', 'down', 'that', 'his', 've', 'how', 'hasn', 'weren', 'is', 'been', 'each', 'wasn', 'up', 'he', 'all', 'having', \"mightn't\", 'while', 'from', 'over', 'my', 'myself', 'after', 'most', 'before', 'haven', 'what', 'for', \"you've\", 'other', 'm', 'both', 'some', 're', 'such', 'do', 'a', 'shouldn', \"doesn't\", 'your', 'where'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PYhwrRRI524z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "20d9b814-cd44-440c-ce6c-8a79f5115d29"
      },
      "cell_type": "code",
      "source": [
        "#Example of Text Vectorization\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# list of text documents\n",
        "text = [\"That quick black cat who jumped over the black lazy dog.\"]\n",
        "# create the transform\n",
        "vectorizer = CountVectorizer(max_features = 5)\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "# encode document\n",
        "vector = vectorizer.transform(text)\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(type(vector))\n",
        "print(vector.toarray())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'black': 0, 'cat': 1, 'jumped': 3, 'lazy': 4, 'dog': 2}\n",
            "(1, 5)\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "[[2 1 1 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}